# 生产问题排查常用命令

如果有线程阻塞的情况，可以使用jstack多抓几次查看线程的栈

```shell
jstack pid > xxx.log
```



如果是cpu占用高，可以通过top命令获取对应的线程id

```shell
top -Hp pid
```

将线程id转为十六进制，然后使用jstack获取栈，在jstak中查找nid=该线程id的线程



如果要查看gc情况，可以使用

```shell
jstat -gcutil pid 时间间隔（毫秒）
```



如果出现了内存溢出，可以抓堆栈，其中live参数是抓当前存活的对象，会触发一次Full GC，要根据实际情况添加

```shell
jmap -dump:live,format=b,file=output.hprof pid
```

也可以在jvm参数中加入参数

```
-XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=./logs/oomDump.hprof
```



如果要进行网络抓包，则可以使用

```shell
tcpdump - ni eth0 port xxxx -w yyy.cap
```





# 实际问题：

## 1、邮件服务健康检查失败导致重启

引用了springboot的actuator健康检查组件后，检查到引用了mail、redis、es等starter后，会定期请求这些服务检查服务是否正常。邮件服务短时间不可用导致mail健康检查失败，导致健康检查整体失败最后重启。





## 2、服务治理的网关开启Spring Cloud Gateway自带的限流后，会出现交易阻塞的情况

这个网关利用redis的发布订阅，来及时更新路由，同时也使用了gateway自带的限流功能。其中有一段特殊的代码，目的是在redis集群发生变化时，重新在新的节点上订阅对应的key，此问题其实是多个极端问题结合才能触发：启用gateway的redis限流，使用gateway的lb负载，限流路由数>=cpu数量

这段代码可以发现是摘抄自网络https://www.cnblogs.com/xfearless/p/11393438.html，核心问题代码如下，其中网关把这里的jedis直接换成了lettuce：

```java
public class RedisMessageListenerFactory implements BeanFactoryAware, ApplicationListener<ContextRefreshedEvent> {

    @Value("${spring.redis.password}")
    private String password;
    
    private DefaultListableBeanFactory beanFactory;

    private RedisConnectionFactory redisConnectionFactory;

    @Autowired
    private MessageListener messageListener;

    public void setBeanFactory(BeanFactory beanFactory) throws BeansException {
        this.beanFactory = (DefaultListableBeanFactory) beanFactory;
    }

    public void setRedisConnectionFactory(RedisConnectionFactory redisConnectionFactory) {
        this.redisConnectionFactory = redisConnectionFactory;
    }

    @Override
    public void onApplicationEvent(ContextRefreshedEvent contextRefreshedEvent) {
        RedisClusterConnection redisClusterConnection = redisConnectionFactory.getClusterConnection();
        if (redisClusterConnection != null) {
            Iterable<RedisClusterNode> nodes = redisClusterConnection.clusterGetNodes();
            for (RedisClusterNode node : nodes) {
                if (node.isMaster()) {
                    String containerBeanName = "messageContainer" + node.hashCode();
                    if (beanFactory.containsBean(containerBeanName)) {
                        return;
                    }
                    JedisShardInfo jedisShardInfo = new JedisShardInfo(node.getHost(), node.getPort());
                    jedisShardInfo.setPassword(password);
                    JedisConnectionFactory factory = new JedisConnectionFactory(jedisShardInfo);
                    BeanDefinitionBuilder containerBeanDefinitionBuilder = BeanDefinitionBuilder
                            .genericBeanDefinition(RedisMessageListenerContainer.class);
                    containerBeanDefinitionBuilder.addPropertyValue("connectionFactory", factory);
                    containerBeanDefinitionBuilder.setScope(BeanDefinition.SCOPE_SINGLETON);
                    containerBeanDefinitionBuilder.setLazyInit(false);
                    beanFactory.registerBeanDefinition(containerBeanName,
                            containerBeanDefinitionBuilder.getRawBeanDefinition());

                    RedisMessageListenerContainer container = beanFactory.getBean(containerBeanName,
                            RedisMessageListenerContainer.class);
                    String listenerBeanName = "messageListener" + node.hashCode();
                    if (beanFactory.containsBean(listenerBeanName)) {
                        return;
                    }
                    container.addMessageListener(messageListener, new PatternTopic("__keyevent@0__:expired"));
                    container.start();
                }
            }
        }
    }

}
```

问题主要部分在`ApplicationListener<ContextRefreshedEvent>`、和一开始的获取sentinel连接上`RedisClusterConnection redisClusterConnection = redisConnectionFactory.getClusterConnection();`。

ContextRefreshedEvent事件在spring的生命周期中是会多次触发的，在spring cloud gateway中，使用lb方式进行负载均衡时，初次触发某个路由会通过synchronized加锁对路由进行初始化并调用Refresh方法刷新容器，最终导致ContextRefreshedEvent事件多次触发。而这个事件内部的建立连接操作用的是lettuce官方不推荐的同步阻塞方式，存在了线程永久阻塞的风险，若不开启Gateway的限流并不会触发此问题。此外Gateway底层使用的reactor netty，lettuce底层也是netty，大概率是netty存在bug，导致请求从reactor的线程在限流查询时切换为lettuce线程后，不会正常切换回来，最终导致lettuce线程去执行获取sentinel连接的操作，而这个操作底层是netty的EventLoopGroup通过轮询对应的lettuce线程去获取，导致同一个线程阻塞等待连接，同时需要该线程去获取，最终该线程无限阻塞，同时gateway创建路由的锁也将无限阻塞，导致最终Gateway接不了新交易。

线程数和cpu相等才会卡的原因：redis限流会用固定的一个lettuce建立连接并以后直接获取，EventLoopGroup轮询机制导致最终该线程轮询cpu次到达自己与自己的卡死情况。





## 3、日志框架由log4j替换为logback

springboot的默认日志框架就是logback

commons-logging + log4j的组合可以被slf4j + logback替换，其中comms-logging和slf4j是接口，而log4j和logback分别是他们的实现

log4j不改代码直接替换为logback的方式：

排除log4j和commons-logging的相关依赖

然后引入如下依赖（版本号根据实际情况修改）：

```xml
<!-- 引入slf4j -->
<dependency>
	<groupId>org.slf4j</groupId>
	<artifactId>slf4j-api</artifactId>
	<version>1.7.32</version>
</dependency>
<!-- 引入logback -->
<dependency>
	<groupId>ch.qos.logback</groupId>
	<artifactId>logback-core</artifactId>
	<version>1.2.9</version>
</dependency>
<dependency>
	<groupId>ch.qos.logback</groupId>
	<artifactId>logback-classic</artifactId>
	<version>1.2.9</version>
</dependency>
<!-- commons-logging的桥接工具，解决删除commons-logging后的类缺失问题 -->
<dependency>
	<groupId>org.slf4j</groupId>
	<artifactId>jcl-over-slf4j</artifactId>
	<version>1.7.32</version>
</dependency>
<!-- log4j的桥接工具，解决删除log4j后的类缺失问题（一般代码中没用接口而是直接使用的实现类才需要这个）-->
<dependency>
	<groupId>org.slf4j</groupId>
	<artifactId>log4j-over-slf4j</artifactId>
	<version>1.7.32</version>
</dependency>
```



## 4、服务正常但nacos中查询不到

nacos-client 1.4.1版本存在bug，在k8s上遇到域名解析超时时，存在心跳永久丢失问题，会导致服务全量下线，升级到1.4.2或更高版本解决。



## 5、Netty服务遇到一些响应时间较长的请求时，会有部分请求跟着超时

该Netty服务端会通过自己构建的Feign进行远程调用，而此操作是同步的，在响应时间较长时，所有该由该线程处理的请求都会阻塞然后出现超时问题。

解决方式是可以将该同步请求的操作改为异步的，包括但不限于使用webclient、改为异步等。

切忌在响应式的框架中写同步阻塞的代码。



## 6、服务占用过多连接数导致K8S主机中Pod宕机问题

现象为k8s主机连接数占满，该主机上工作负载的健康检查无法通过并开始重启。

原因为某个服务因为业务需求，有一个间隔3s的定时任务连接至Kafka服务获取信息，并且每次都是重新创建的连接对象且没有关闭。



大致问题代码如下：

```java
@EnableScheduling
public class KafkaDataIncressDataScheduled{
	private final KafkaAdminClient kafkaAdminClient;
	private finale Properties properties;
	
	public KafkaDataIncressDataScheduled() {
        Map<String, Object> props = new HashMap<>(2);
        props.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, "ip:port,ip:port");
        kafkaAdminClient = (kafkaAdminClient) AdminClient.create(props);
        
        
        properties = new Properties();
        properties.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "ip:port,ip:port");
        properties.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, "org.apache.kafka.common.serialization.StringDeserializer");
        properties.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, "org.apache.kafka.common.serialization.StringDeserializer");
        properties.put(ConsumerConfig.GROUP_ID_CONFIG, "testId");
    }

	@Scheduled(initialDelay = 0, fixedDelay = 3000)
    public void loadKafkaCount() throws Exception {
        Map<TopicPartition, OffsetAndMetadata> map = kafkaAdminClient.listConsumerGroupOffsets("testId").partitionsToOffsetAndMetadata().get();
        
        // 问题就在这里了，每次都重新new了一个KafkaConsumer，光new并不会建立连接，执行对应的方法时才会
        KafkaConsumer kafkaConsumer = new KafkaConsumer(properties);
        for(TopicPartition topicPartition : map.keySet()){
            Map<TopicPartition, Long> endOffsets = kafkaConsumer.endOffsets(Collections.singletonList(topicPartition));
			Map<TopicPartition, OffsetAndMetadata> committedMap = kafkaConsumer.committed(Collections.singletonList(topicPartition));
        }
    }
}
```



* 该服务因为问题代码，连接数每3秒涨5个，5个具体如下：

1. 执行endOffsets方法，kafkaConsumer连接至任意一台，获取对应partition所在的broker
2. 继续执行endOffsets方法，连接至该broker（即使与第1个连接是同一个ip也会重新建立连接），并执行对应的逻辑
3. 执行committed方法，连接至协调者（获取协调者ip的操作在哪个连接暂不确定），执行该方法具体逻辑
4. for循环继续执行endOffsets方法，此时的partition与第2步所在的broker不是一个，连接至该broker并执行对应逻辑
5. for循环继续执行endOffsets方法，此时的partition与第2、4步所在的broker不是一个，连接至该broker并执行对应逻辑

本服务的kafka有3个broker，且topic的partition数大于等于3，所以连接数会增长1+1+broker数量个



虽然kafkaConsumer默认在9分钟时会主动关闭连接，但自己new出来的不会有这个效果。

在kafkaConsumer与服务端连接到达10分钟时，服务端会主动发送Fin与客户端断开连接，但客户端这边不会响应任何Fin，此时该tcp连接进入CLOSE_WAIT状态。

CLOSE_WAIT会维持`tcp_keepalive_time + tcp_keepalive_intvl * tcp_keepalive_probes` 的时间，在linux的默认配置下，` tcp_keepalive_time + tcp_keepalive_intvl * tcp_keepalive_probes = 7200 + 75 * 9 = 7875秒`，所以连接数在大约两小时后会趋于稳定



解决方法：手动调用kafkaConsumer.close()关闭连接



## 7、Redis服务节点重启后lettuce出现若干问题

### 7.1、K8S部署的redis集群模式

复现方式：在K8S部署集群模式redis（节点重启后大概率ip地址会发生变化）

1. 服务set key，并查询该key对应插槽所在的主节点
2. 干掉对应的主节点，且保证节点重启后<font color="red">ip发生变化</font>
3. 再次set 相同的key即可复现



出现原因：spring-boot-starter-data-redis（lettuce）默认是不开启拓扑刷新的，所以当节点ip发生变化后，服务依旧会把对应插槽的数据向旧节点发送，所以会出现错误



解决方案：在配置文件中添加拓扑刷新配置（springboot2.3后），两个策略是独立运行的

```yaml
spring:
	redis:
		lettuce:
			cluster:
				refresh:
					# 开启主动刷新，在收到-ASK和-MOVED到达5次后刷新拓扑
					adaptive: true
					# 开启定时刷新，每隔xx毫秒刷新一次拓扑
					period: 30000
```

更多lettuce配置见：https://github.com/lettuce-io/lettuce-core/wiki/Redis-Cluster#user-content-refreshing-the-cluster-topology-view



### 7.2、哨兵模式

哨兵模式下，若主节点在K8S中快速重启发生主从切换且<font color="red">ip没有发生变化</font>，或者直接在主节点中使用slaveof命令切换主从应用，此时应用无法感知节点发生变化，并会继续将写请求发送至原来的主节点（现在为从节点），然后收到报错read only。



解决方法是指定哨兵的读写策略，任意策略都可以，未指定时不会自动拓扑刷新，而指定了则会

```java
@Bean
public LettuceClientConfigurationBuilderCustomizer lettuceClientConfigurationBuilderCustomizer() {
	return builder -> builder.readFrom(ReadFrom.REPLICA);
}
```



## 8、信创主机下服务日志打印阻塞导致所有交易卡住

正常输出大量日志的时候可能出现，jstack中日志线程其实是RUNNABLE的，但在操作系统层面阻塞住了，内核版本为基于4.19.20修改的麒麟内核，社区开源版在高版本中修复了此问题

连接如下：https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=v6.8-rc1&id=4903fde8047a28299d1fc79c1a0dcc255e928f12
