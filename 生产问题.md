# 生产问题排查常用命令

如果有线程阻塞的情况，可以使用jstack多抓几次查看线程的栈

```shell
jstack pid > xxx.log
```



如果是cpu占用高，可以通过top命令获取对应的线程id

```shell
top -Hp pid
```

将线程id转为十六进制，然后使用jstack获取栈，在jstak中查找nid=该线程id的线程



如果要查看gc情况，可以使用

```shell
jstat -gcutil pid 时间间隔（毫秒）
```



如果出现了内存溢出，可以抓堆栈，其中live参数是抓当前存活的对象，会触发一次Full GC，要根据实际情况添加

```shell
jmap -dump:live,format=b,file=output.hprof pid
```

也可以在jvm参数中加入参数

```
-XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=./logs/oomDump.hprof
```



如果要进行网络抓包，则可以使用

```shell
tcpdump - ni eth0 port xxxx -w yyy.cap
```





# 实际问题：

## 1、邮件服务健康检查失败导致重启

引用了springboot的actuator健康检查组件后，检查到引用了mail、redis、es等starter后，会定期请求这些服务检查服务是否正常。邮件服务短时间不可用导致mail健康检查失败，导致健康检查整体失败最后重启。





## 2、服务治理的网关开启Spring Cloud Gateway自带的限流后，会出现交易阻塞的情况

这个网关利用redis的发布订阅，来及时更新路由，同时也使用了gateway自带的限流功能。其中有一段特殊的代码，目的是在redis集群发生变化时，重新在新的节点上订阅对应的key，此问题其实是多个极端问题结合才能触发：启用gateway的redis限流，使用gateway的lb负载，限流路由数>=cpu数量

这段代码可以发现是摘抄自网络https://www.cnblogs.com/xfearless/p/11393438.html，核心问题代码如下，其中网关把这里的jedis直接换成了lettuce：

```java
public class RedisMessageListenerFactory implements BeanFactoryAware, ApplicationListener<ContextRefreshedEvent> {

    @Value("${spring.redis.password}")
    private String password;
    
    private DefaultListableBeanFactory beanFactory;

    private RedisConnectionFactory redisConnectionFactory;

    @Autowired
    private MessageListener messageListener;

    public void setBeanFactory(BeanFactory beanFactory) throws BeansException {
        this.beanFactory = (DefaultListableBeanFactory) beanFactory;
    }

    public void setRedisConnectionFactory(RedisConnectionFactory redisConnectionFactory) {
        this.redisConnectionFactory = redisConnectionFactory;
    }

    @Override
    public void onApplicationEvent(ContextRefreshedEvent contextRefreshedEvent) {
        RedisClusterConnection redisClusterConnection = redisConnectionFactory.getClusterConnection();
        if (redisClusterConnection != null) {
            Iterable<RedisClusterNode> nodes = redisClusterConnection.clusterGetNodes();
            for (RedisClusterNode node : nodes) {
                if (node.isMaster()) {
                    String containerBeanName = "messageContainer" + node.hashCode();
                    if (beanFactory.containsBean(containerBeanName)) {
                        return;
                    }
                    JedisShardInfo jedisShardInfo = new JedisShardInfo(node.getHost(), node.getPort());
                    jedisShardInfo.setPassword(password);
                    JedisConnectionFactory factory = new JedisConnectionFactory(jedisShardInfo);
                    BeanDefinitionBuilder containerBeanDefinitionBuilder = BeanDefinitionBuilder
                            .genericBeanDefinition(RedisMessageListenerContainer.class);
                    containerBeanDefinitionBuilder.addPropertyValue("connectionFactory", factory);
                    containerBeanDefinitionBuilder.setScope(BeanDefinition.SCOPE_SINGLETON);
                    containerBeanDefinitionBuilder.setLazyInit(false);
                    beanFactory.registerBeanDefinition(containerBeanName,
                            containerBeanDefinitionBuilder.getRawBeanDefinition());

                    RedisMessageListenerContainer container = beanFactory.getBean(containerBeanName,
                            RedisMessageListenerContainer.class);
                    String listenerBeanName = "messageListener" + node.hashCode();
                    if (beanFactory.containsBean(listenerBeanName)) {
                        return;
                    }
                    container.addMessageListener(messageListener, new PatternTopic("__keyevent@0__:expired"));
                    container.start();
                }
            }
        }
    }

}
```

问题主要部分在`ApplicationListener<ContextRefreshedEvent>`、和一开始的获取sentinel连接上`RedisClusterConnection redisClusterConnection = redisConnectionFactory.getClusterConnection();`。

ContextRefreshedEvent事件在spring的生命周期中是会多次触发的，在spring cloud gateway中，使用lb方式进行负载均衡时，初次触发某个路由会通过synchronized加锁对路由进行初始化并调用Refresh方法刷新容器，最终导致ContextRefreshedEvent事件多次触发。而这个事件内部的建立连接操作用的是lettuce官方不推荐的同步阻塞方式，存在了线程永久阻塞的风险，若不开启Gateway的限流并不会触发此问题。此外Gateway底层使用的reactor netty，lettuce底层也是netty，大概率是netty存在bug，导致请求从reactor的线程在限流查询时切换为lettuce线程后，不会正常切换回来，最终导致lettuce线程去执行获取sentinel连接的操作，而这个操作底层是netty的EventLoopGroup通过轮询对应的lettuce线程去获取，导致同一个线程阻塞等待连接，同时需要该线程去获取，最终该线程无限阻塞，同时gateway创建路由的锁也将无限阻塞，导致最终Gateway接不了新交易。

线程数和cpu相等才会卡的原因：redis限流会用固定的一个lettuce建立连接并以后直接获取，EventLoopGroup轮询机制导致最终该线程轮询cpu次到达自己与自己的卡死情况。





## 3、日志框架由log4j替换为logback

springboot的默认日志框架就是logback

commons-logging + log4j的组合可以被slf4j + logback替换，其中comms-logging和slf4j是接口，而log4j和logback分别是他们的实现

log4j不改代码直接替换为logback的方式：

排除log4j和commons-logging的相关依赖

然后引入如下依赖（版本号根据实际情况修改）：

```xml
<!-- 引入slf4j -->
<dependency>
	<groupId>org.slf4j</groupId>
	<artifactId>slf4j-api</artifactId>
	<version>1.7.32</version>
</dependency>
<!-- 引入logback -->
<dependency>
	<groupId>ch.qos.logback</groupId>
	<artifactId>logback-core</artifactId>
	<version>1.2.9</version>
</dependency>
<dependency>
	<groupId>ch.qos.logback</groupId>
	<artifactId>logback-classic</artifactId>
	<version>1.2.9</version>
</dependency>
<!-- commons-logging的桥接工具，解决删除commons-logging后的类缺失问题 -->
<dependency>
	<groupId>org.slf4j</groupId>
	<artifactId>jcl-over-slf4j</artifactId>
	<version>1.7.32</version>
</dependency>
<!-- log4j的桥接工具，解决删除log4j后的类缺失问题（一般代码中没用接口而是直接使用的实现类才需要这个）-->
<dependency>
	<groupId>org.slf4j</groupId>
	<artifactId>log4j-over-slf4j</artifactId>
	<version>1.7.32</version>
</dependency>
```



## 4、服务正常但nacos中查询不到

nacos-client 1.4.1版本存在bug，在k8s上遇到域名解析超时时，存在心跳永久丢失问题，会导致服务全量下线，升级到1.4.2或更高版本解决。



## 5、Netty服务遇到一些响应时间较长的请求时，会有部分请求跟着超时

该Netty服务端会通过自己构建的Feign进行远程调用，而此操作是同步的，在响应时间较长时，所有该由该线程处理的请求都会阻塞然后出现超时问题。

解决方式是可以将该同步请求的操作改为异步的，包括但不限于使用webclient、改为异步等。

切忌在响应式的框架中写同步阻塞的代码。



## 6、服务占用过多连接数导致K8S主机中Pod宕机问题

现象为k8s主机连接数占满，该主机上工作负载的健康检查无法通过并开始重启。

原因为某个服务因为业务需求，有一个间隔3s的定时任务连接至Kafka服务获取信息，并且每次都是重新创建的连接对象且没有关闭。



大致问题代码如下：

```java
@EnableScheduling
public class KafkaDataIncressDataScheduled{
	private final KafkaAdminClient kafkaAdminClient;
	private finale Properties properties;
	
	public KafkaDataIncressDataScheduled() {
        Map<String, Object> props = new HashMap<>(2);
        props.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, "ip:port,ip:port");
        kafkaAdminClient = (kafkaAdminClient) AdminClient.create(props);
        
        
        properties = new Properties();
        properties.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "ip:port,ip:port");
        properties.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, "org.apache.kafka.common.serialization.StringDeserializer");
        properties.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, "org.apache.kafka.common.serialization.StringDeserializer");
        properties.put(ConsumerConfig.GROUP_ID_CONFIG, "testId");
    }

	@Scheduled(initialDelay = 0, fixedDelay = 3000)
    public void loadKafkaCount() throws Exception {
        Map<TopicPartition, OffsetAndMetadata> map = kafkaAdminClient.listConsumerGroupOffsets("testId").partitionsToOffsetAndMetadata().get();
        
        // 问题就在这里了，每次都重新new了一个KafkaConsumer，光new并不会建立连接，执行对应的方法时才会
        KafkaConsumer kafkaConsumer = new KafkaConsumer(properties);
        for(TopicPartition topicPartition : map.keySet()){
            Map<TopicPartition, Long> endOffsets = kafkaConsumer.endOffsets(Collections.singletonList(topicPartition));
			Map<TopicPartition, OffsetAndMetadata> committedMap = kafkaConsumer.committed(Collections.singletonList(topicPartition));
        }
    }
}
```



* 该服务因为问题代码，连接数每3秒涨5个，5个具体如下：

1. 执行endOffsets方法，kafkaConsumer连接至任意一台，获取对应partition所在的broker
2. 继续执行endOffsets方法，连接至该broker（即使与第1个连接是同一个ip也会重新建立连接），并执行对应的逻辑
3. 执行committed方法，连接至协调者（获取协调者ip的操作在哪个连接暂不确定），执行该方法具体逻辑
4. for循环继续执行endOffsets方法，此时的partition与第2步所在的broker不是一个，连接至该broker并执行对应逻辑
5. for循环继续执行endOffsets方法，此时的partition与第2、4步所在的broker不是一个，连接至该broker并执行对应逻辑

本服务的kafka有3个broker，且topic的partition数大于等于3，所以连接数会增长1+1+broker数量个



虽然kafkaConsumer默认在9分钟时会主动关闭连接，但自己new出来的不会有这个效果。

在kafkaConsumer与服务端连接到达10分钟时，服务端会主动发送Fin与客户端断开连接，但客户端这边不会响应任何Fin，此时该tcp连接进入CLOSE_WAIT状态。

CLOSE_WAIT会维持`tcp_keepalive_time + tcp_keepalive_intvl * tcp_keepalive_probes` 的时间，在linux的默认配置下，` tcp_keepalive_time + tcp_keepalive_intvl * tcp_keepalive_probes = 7200 + 75 * 9 = 7875秒`，所以连接数在大约两小时后会趋于稳定



解决方法：手动调用kafkaConsumer.close()关闭连接



## 7、Redis服务节点重启后lettuce出现若干问题

### 7.1、K8S部署的redis集群模式

复现方式：在K8S部署集群模式redis（节点重启后大概率ip地址会发生变化）

1. 服务set key，并查询该key对应插槽所在的主节点
2. 干掉对应的主节点，且保证节点重启后<font color="red">ip发生变化</font>
3. 再次set 相同的key即可复现



出现原因：spring-boot-starter-data-redis（lettuce）默认是不开启拓扑刷新的，所以当节点ip发生变化后，服务依旧会把对应插槽的数据向旧节点发送，所以会出现错误



解决方案：在配置文件中添加拓扑刷新配置（springboot2.3后），两个策略是独立运行的

```yaml
spring:
	redis:
		lettuce:
			cluster:
				refresh:
					# 开启主动刷新，在收到-ASK和-MOVED到达5次后刷新拓扑
					adaptive: true
					# 开启定时刷新，每隔xx毫秒刷新一次拓扑
					period: 30000
```

更多lettuce配置见：https://github.com/lettuce-io/lettuce-core/wiki/Redis-Cluster#user-content-refreshing-the-cluster-topology-view



### 7.2、哨兵模式

哨兵模式下，若主节点在K8S中快速重启发生主从切换且<font color="red">ip没有发生变化</font>，或者直接在主节点中使用slaveof命令切换主从应用，此时应用无法感知节点发生变化，并会继续将写请求发送至原来的主节点（现在为从节点），然后收到报错read only。



解决方法是指定哨兵的读写策略，任意策略都可以，未指定时不会自动拓扑刷新，而指定了则会

```java
@Bean
public LettuceClientConfigurationBuilderCustomizer lettuceClientConfigurationBuilderCustomizer() {
	return builder -> builder.readFrom(ReadFrom.REPLICA);
}
```



## 8、信创主机下服务日志打印阻塞导致所有交易卡住

正常输出大量日志的时候可能出现，jstack中日志线程其实是RUNNABLE的，但在操作系统层面阻塞住了，内核版本为基于4.19.20修改的麒麟内核，社区开源版在高版本中修复了此问题

连接如下：https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=v6.8-rc1&id=4903fde8047a28299d1fc79c1a0dcc255e928f12





## 9、基于redis的分布式序列增加DNS切换能力

### 9.1、哨兵模式增加拓扑刷新能力

lettuce的默认哨兵模式没有拓扑刷新能力，因为应用连接redis的流程为：

1. 连接哨兵
2. 获取redis节点信息
3. 断开与哨兵的连接
4. 连接redis节点

通过设置使用主节点读，开启lettuce自带的拓扑刷新，此时应用会额外建立一个与哨兵的长连接，监听哨兵的相关事件，当发生主从切换后会感知到，后续使用新的主进行操作。（也能解决如果从节点开起了READONLY模式后，发生主从切换后应用无法感知，使用从主节点进行写，然后报Read Only的问题）

```java
@Bean
public LettuceClientConfigurationBuilderCustomizer lettuceClientConfigurationBuilderCustomizer() {
    return builder -> builder.readFrom(ReadFrom.MASTER);
}
```



### 9.2、lettuce卡住15分钟问题

当Redis与客户端之间的网络链路**非正常断开**（例如网络彻底断开、云平台强制删除redis节点）导致无法发出FIN包后，客户端的TCP连接会卡在 **“半开”（Half-Open）状态**；当客户端使用僵尸连接发送命令时也不会收到ACK，这时，**Linux内核的TCP协议栈会启动超时重传机制**。内核会按照指数退避的原则，在第一次重传、第二次、第三次...之间等待越来越长的时间，不断尝试重发这个数据包，希望能重建连接。

**关键的15分钟就来源于此**。在Linux系统中，控制放弃前最多重试次数的核心参数是 `tcp_retries2`（默认值为**15**）。根据内核设定的重传时间算法，从第一次发送到重试15次后最终放弃，整个过程为15分钟左右



因为序列为云上应用，不便于修改主机、容器的操作系统配置，所以考虑从应用侧修改

1. lettuce官方在高版本中增加了`TCP_USER_TIMEOUT`配置解决此问题，

```java
@Bean
public LettuceClientConfigurationBuilderCustomizer lettuceCustomizer() {
    return clientConfigurationBuilder -> {
        clientConfigurationBuilder
            .socketOptions(SocketOptions.builder()
                .tcpUserTimeout(TcpUserTimeoutOptions.enable(Duration.ofSeconds(30))) // 设置为30秒
                .build());
    };
}
```



2. 但我们的框架依赖版本是统一控制的，对于旧的框架版本来说，修改中间件依赖的版本风险很大，所以我们选择在应用内直接进行修改，因为lettuce是基于netty的，所以我们在其中增加了空闲检测和异常检测的handler，在空闲1分钟或出现连接异常时断开连接，然后连接会由watchDog看门狗线程自动重连

```java
@Bean
public ClientResources clientResources() {
    NettyCustomizer nettyCustomizer = new NettyCustomizer() {
        @Override
        public void afterChannelInitialized(Channel ch) {
            ch.pipeline()
                    .addLast(new IdleStateHandler(0, 0, 60))
                    .addLast(new ChannelDuplexHandler() {
                        @Override
                        public void userEventTriggered(ChannelHandlerContext ctx, Object evt) throws Exception {
                            // 在lettuce6.3.0版本有修复此问题，但需要额外配置
                            // 通过闲置重连方式解决redis节点宕机未返回fin导致连接存在15分钟问题，由WatchDog自动尝试重连
                            if (evt instanceof IdleStateEvent) {
                                ctx.close();
                            }
                        }

                        @Override
                        public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception {
                            // 连接出错时也断开连接，由WatchDog自动尝试重连
                            ctx.close();
                        }
                    });
        }
    };
    return ClientResources.builder().nettyCustomizer(nettyCustomizer).build();
}
```

此解决方案长期稳定，仅出现过一次监控告警：具体情况为获取序列为毫秒级，而项目组配了获取序列超过50ms一次时告警，而那次碰巧是redis连接闲置断开时收到获取序列请求，redis连接建立耗时大约50ms，实际redis仅1ms，所以触发监控告警。



### 9.3、自动切换DNS功能

银行架构为两地三中心，所以需要序列有切换中心的能力。因为数据库为分布式数据库，行方要求应用有切换redis的能力。

开始情况为切换redis的DNS后，应用因为9.2改造中增加的异常handler自动重连，会连接到新中心的哨兵，但仍会不停重连旧中心的redis节点。经过查看源码及官方github得出结论：官方认为哨兵模式lettuce的一切拓扑刷新都要依赖于订阅哨兵节点的消息实现（参考官方的回答https://github.com/redis/lettuce/issues/2007），而在dns切换过程中两个中心的哨兵并没有产生主从切换等需要重新建立连接的事件，所以redis连接没有被更新，应用仍然尝试连接旧的redis节点。



具体为在每次连接哨兵时记录当前哨兵集群IP，如果哨兵集群IP的网段发生变化（DNS切换），则触发拓扑刷新

拓扑实现方案有两种：

* 一种是覆盖lettuce源码，lettuce拓扑刷新代码为default的，将其改为public获得直接调用能力

  * 优点：刷新过程是同步阻塞的，可在完成后马上进行我们想要的额外操作，例如清空当前redis存的序列信息，从数据库重新获取保证不重复
  * 缺点：改的内容相对比较核心，未来源码可能更新，相对不太友好

  

* 另一种是在lettuce底层的netty中添加handler，自己模拟哨兵发送触发拓扑刷新的消息。
  * 优点：无侵入
  * 缺点：此方式是异步的，所以需要进一步修改部分官方不用的源码来通知进行额外操作（例如清空当前redis存的序列信息，从数据库重新获取保证不重复

